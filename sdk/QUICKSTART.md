# SDK Quick Start Guide

Get started with MLE Runtime in your preferred language in under 5 minutes.

## Choose Your Language

- [Node.js / TypeScript](#nodejs--typescript)
- [Java](#java)
- [Python](#python)
- [C/C++](#cc)

---

## Node.js / TypeScript

### Install
```bash
npm install @mle/runtime
```

### Basic Usage
```typescript
import { MLEEngine, Device } from '@mle/runtime';

const engine = new MLEEngine(Device.CPU);
await engine.loadModel('model.mle');

const input = new Float32Array([1, 2, 3, 4]);
const outputs = await engine.run([input]);

console.log('Prediction:', outputs[0]);
engine.dispose();
```

### Run Example
```bash
cd sdk/nodejs
npm install
npm run build
node examples/simple_inference.js path/to/model.mle
```

[Full Documentation](nodejs/README.md)

---

## Java

### Install (Maven)
```xml
<dependency>
    <groupId>com.mle</groupId>
    <artifactId>mle-runtime</artifactId>
    <version>1.0.0</version>
</dependency>
```

### Basic Usage
```java
import com.mle.runtime.*;

try (MLEEngine engine = new MLEEngine(Device.CPU)) {
    engine.loadModel("model.mle");
    
    float[] input = {1.0f, 2.0f, 3.0f, 4.0f};
    List<float[]> outputs = engine.run(List.of(input));
    
    System.out.println("Prediction: " + outputs.get(0)[0]);
}
```

### Run Example
```bash
cd sdk/java
mvn clean install
java -cp target/mle-runtime-1.0.0.jar SimpleInference path/to/model.mle
```

[Full Documentation](java/README.md)

---

## Python

### Install
```bash
pip install mle-runtime
```

### Basic Usage
```python
import mle_runtime
import numpy as np

engine = mle_runtime.MLEEngine(mle_runtime.Device.CPU)
engine.load_model("model.mle")

input_data = np.array([[1, 2, 3, 4]], dtype=np.float32)
outputs = engine.run([input_data])

print("Prediction:", outputs[0])
```

### Run Example
```bash
cd sdk/python
pip install -e .
python examples/simple_inference.py path/to/model.mle
```

[Full Documentation](python/README.md)

---

## C/C++

### Install
```bash
# Copy header
cp sdk/cpp/include/mle_client.h /usr/local/include/

# Link against libmle_core
g++ -std=c++20 main.cpp -lmle_core -o app
```

### Basic Usage
```cpp
#include "mle_client.h"

int main() {
    mle::MLEEngine engine(mle::Device::CPU);
    engine.load_model("model.mle");
    
    auto input = std::make_shared<mle::Tensor>(
        std::vector<uint32_t>{1, 4}, 
        mle::DType::FP32
    );
    
    auto outputs = engine.run({input});
    
    return 0;
}
```

### Run Example
```bash
cd sdk/cpp
mkdir build && cd build
cmake ..
make
./simple_inference path/to/model.mle
```

[Full Documentation](cpp/README.md)

---

## Next Steps

1. **Export a model**: Use the exporters in `tools/exporter/`
2. **Run benchmarks**: Compare with joblib/pickle
3. **Deploy to production**: See deployment guides in each SDK
4. **Explore features**: Compression, signing, versioning

## Performance

All SDKs provide similar performance:

| Metric | Traditional | MLE | Improvement |
|--------|------------|-----|-------------|
| Load Time | 100-500ms | 1-5ms | **100x faster** |
| File Size | 100MB | 20MB | **80% smaller** |
| Inference | Python/JVM | Native C++ | **2-5x faster** |

## Support

- üìñ Documentation: See individual SDK READMEs
- üêõ Issues: GitHub Issues
- üí¨ Discussions: GitHub Discussions

---

**Choose your language and start building!** üöÄ
